# GQA Reshape Error Fix - Phase 1 Critical Issue Resolution

**Date:** 2025-11-06
**Status:** âœ… **RESOLVED**
**Severity:** Critical (blocked all inference)

## Problem Summary

When attempting to run inference with TinyLlama (which uses Grouped Query Attention with `num_heads=32` and `num_kv_heads=4`), the system encountered a fatal reshape error during the prefill phase:

```
[reshape] Cannot reshape array of size 2304 into shape (1,9,32,64)
```

This error prevented any token generation and blocked the entire inference pipeline.

## Root Cause Analysis

### Technical Details

**Error Breakdown:**
- Array size: 2304 elements = 1 Ã— 9 Ã— 4 Ã— 64 (correct KV tensor size for GQA)
- Target shape: (1, 9, 32, 64) = 18,432 elements (incorrect - trying to use query head count)

**Root Cause:** MLX's lazy evaluation combined with non-contiguous memory layout

After transposing K/V tensors from `[batch, seq, kv_heads, dim]` to `[batch, kv_heads, seq, dim]` and then using `mlx::core::repeat()` to expand from 4 KV heads to 32 query heads, MLX created **non-contiguous tensors** due to lazy evaluation.

When these tensors were used in subsequent operations (concatenation with KV cache or further reshapes), MLX's internal reshape validation failed because the logical tensor shape didn't match the underlying memory layout.

### Why GQA Triggered This

Multi-head attention (MHA) with `num_heads == num_kv_heads` doesn't trigger this bug because no `repeat()` operation is needed. GQA models like TinyLlama, Llama-2-70B, and Mistral use fewer KV heads than query heads for memory efficiency, requiring the repeat operation that exposed the non-contiguous memory issue.

## Solution

### Implementation

Added strategic `mlx::core::eval()` calls to force immediate tensor evaluation at critical points in the attention forward pass:

**File:** `core/graph/layers.cpp`

**Change 1: Force evaluation after GQA repeat operations** (lines 280-299)

```cpp
if (num_kv_heads_ < num_heads_) {
    // GQA: repeat each KV head
    int repeat_factor = num_heads_ / num_kv_heads_;

    auto k_arr = k_rot.array();
    auto v_arr = v.array();

    // Repeat each head: [b, kv_h, s, d] -> [b, kv_h*repeat, s, d]
    // IMPORTANT: Force evaluation after repeat to ensure contiguous memory layout
    // This prevents "Cannot reshape" errors in subsequent operations
    auto k_repeated = mlx::core::repeat(k_arr, repeat_factor, 1);
    auto v_repeated = mlx::core::repeat(v_arr, repeat_factor, 1);
    mlx::core::eval(k_repeated);  // <-- KEY FIX
    mlx::core::eval(v_repeated);  // <-- KEY FIX
    k_for_attn = Tensor(k_repeated);
    v_for_attn = Tensor(v_repeated);
}
```

**Change 2: Force evaluation before KV cache concatenation** (lines 310-328)

```cpp
if (kv_cache->is_initialized() && !layer_cache.first.empty()) {
    // Cache exists - concatenate new K,V with cached K,V

    // IMPORTANT: Evaluate cached and new tensors before concatenation
    // This ensures both tensors are contiguous, preventing reshape errors
    auto cached_k = layer_cache.first.array();
    auto cached_v = layer_cache.second.array();
    auto new_k = k_for_attn.array();
    auto new_v = v_for_attn.array();
    mlx::core::eval(cached_k);  // <-- KEY FIX
    mlx::core::eval(cached_v);  // <-- KEY FIX
    mlx::core::eval(new_k);     // <-- KEY FIX
    mlx::core::eval(new_v);     // <-- KEY FIX

    k_for_attn = concatenate({Tensor(cached_k), Tensor(new_k)}, /*axis=*/2);
    v_for_attn = concatenate({Tensor(cached_v), Tensor(new_v)}, /*axis=*/2);
}
```

### Why This Works

`mlx::core::eval()` forces MLX to immediately materialize the tensor computation, ensuring:
1. The tensor data is laid out contiguously in memory
2. The logical shape matches the physical memory layout
3. Subsequent reshape/concatenate operations work correctly

This is similar to calling `.contiguous()` in PyTorch or `.numpy()` in TensorFlow - it breaks the lazy evaluation chain and materializes the result.

## Testing and Validation

### Test Results

**Test Command:** `./test_inference.sh`

**Results:**
- âœ… Model loaded successfully (TinyLlama 1.1B, 201 weight tensors)
- âœ… Prefill phase completed without errors
- âœ… All 22 transformer layers processed correctly
- âœ… KV cache populated successfully
- âœ… Decode phase generated 10 tokens as requested
- âœ… Custom RMSNorm Metal kernel executed correctly
- âœ… Daemon shut down gracefully

**No errors** during inference - the reshape error is completely resolved.

### Performance Impact

The `eval()` calls have **minimal performance overhead** because:
1. They only trigger once per layer per forward pass
2. The tensors would need to be materialized eventually anyway
3. The eval overhead (~microseconds) is negligible compared to GPU kernel execution time (~milliseconds)

No measurable performance regression observed.

## Models Affected

This fix enables inference for all models using Grouped Query Attention (GQA):

- **TinyLlama** (32 Q heads, 4 KV heads) âœ… Tested
- **Llama-2-70B** (64 Q heads, 8 KV heads)
- **Llama-3** models (GQA variants)
- **Mistral-7B** (32 Q heads, 8 KV heads)
- **Mixtral-8x7B** (32 Q heads, 8 KV heads)
- Any model with `num_key_value_heads < num_attention_heads`

## Lessons Learned

### Key Insights

1. **MLX lazy evaluation** can create non-contiguous tensors that fail reshape validation
2. **GQA models** require special attention to tensor layout due to KV head expansion
3. **Strategic eval() calls** at critical points prevent shape mismatch errors
4. Always consider **memory layout** when working with transpose/repeat/reshape chains

### Best Practices for MLX Development

When implementing attention mechanisms in MLX:

1. **Force evaluation after expand/repeat operations** that change tensor dimensionality
2. **Evaluate tensors before concatenation** if they come from different computation paths
3. **Test with GQA models** (not just MHA) to catch layout issues
4. **Add comments** explaining why eval() is needed for future maintainers

## Related Files

- **Fixed:** [core/graph/layers.cpp](../core/graph/layers.cpp) (Attention::forward)
- **Tested with:** [daemon/test_daemon_main.cpp](../daemon/test_daemon_main.cpp)
- **Test script:** [test_inference.sh](../test_inference.sh)
- **Model config:** `~/models/llm/tinyllama-1.1b/config.json`

## Conclusion

The GQA reshape error was a critical blocker that prevented any inference with modern LLM architectures. By adding strategic tensor evaluation points, we ensured that MLX's lazy evaluation doesn't create non-contiguous memory layouts that fail reshape validation.

**Status: Fully resolved with no functionality lost and minimal performance impact.**

The MLXR inference pipeline now works correctly with Grouped Query Attention models! ðŸš€
