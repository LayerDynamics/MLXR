# TinyLlama 1.1B Model Configuration
# https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0

model:
  name: "TinyLlama-1.1B-Chat-v1.0"
  family: "llama"
  architecture: "llama"

  # Model file paths
  path: "~/Library/Application Support/MLXRunner/models/TinyLlama-1.1B-Chat-v1.0"
  format: "safetensors"  # safetensors, gguf, or mlx

  # Alternative: GGUF format
  # path: "~/Library/Application Support/MLXRunner/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
  # format: "gguf"

# Model Architecture Parameters
architecture:
  vocab_size: 32000
  hidden_size: 2048
  num_hidden_layers: 22
  num_attention_heads: 32
  num_key_value_heads: 4  # GQA (Grouped Query Attention)
  intermediate_size: 5632  # MLP hidden dimension
  max_position_embeddings: 2048
  rms_norm_eps: 1.0e-5
  rope_theta: 10000.0

# Tokenizer Configuration
tokenizer:
  type: "sentencepiece"
  path: "~/Library/Application Support/MLXRunner/models/TinyLlama-1.1B-Chat-v1.0/tokenizer.model"

  # Special tokens
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"
  pad_token: "</s>"

# Quantization Settings
quantization:
  dtype: "q4_k"  # FP16, Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, Q8_K
  group_size: 128  # For K-quants

# Context & Generation
context:
  max_length: 2048
  default_max_new_tokens: 512

# RoPE Scaling (for extended context)
rope_scaling:
  type: "base"  # base, ntk, or yarn
  factor: 1.0

# Chat Template
chat_template: |
  <|system|>
  {{ system_message }}</s>
  <|user|>
  {{ user_message }}</s>
  <|assistant|>

# System Prompt
default_system_prompt: "You are a helpful AI assistant."

# Sampling Presets
sampling_presets:
  creative:
    temperature: 0.9
    top_p: 0.95
    top_k: 50
    repetition_penalty: 1.1

  balanced:
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    repetition_penalty: 1.1

  precise:
    temperature: 0.3
    top_p: 0.85
    top_k: 20
    repetition_penalty: 1.15

# Performance Hints
performance:
  # Recommended batch size for this model
  optimal_batch_size: 4

  # Memory estimates (approximate)
  memory_fp16_mb: 2200
  memory_q4_k_mb: 700
  memory_q8_k_mb: 1200

  # Performance targets (M4)
  target_prefill_ms: 150
  target_decode_ms: 50

# Model Metadata
metadata:
  description: "TinyLlama 1.1B Chat - Compact conversational model"
  license: "Apache 2.0"
  paper: "https://arxiv.org/abs/2401.02385"
  huggingface: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  version: "v1.0"
  release_date: "2024-01"

# Tags for organization
tags:
  - "chat"
  - "small"
  - "fast"
  - "gqa"
