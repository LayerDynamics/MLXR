# Llama 3 8B Model Configuration
# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct

model:
  name: "Meta-Llama-3-8B-Instruct"
  family: "llama"
  architecture: "llama"

  # Model file paths
  path: "~/Library/Application Support/MLXRunner/models/Meta-Llama-3-8B-Instruct"
  format: "safetensors"  # safetensors, gguf, or mlx

  # Alternative: GGUF format
  # path: "~/Library/Application Support/MLXRunner/models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
  # format: "gguf"

# Model Architecture Parameters
architecture:
  vocab_size: 128256
  hidden_size: 4096
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 8  # GQA (Grouped Query Attention)
  intermediate_size: 14336  # MLP hidden dimension
  max_position_embeddings: 8192
  rms_norm_eps: 1.0e-5
  rope_theta: 500000.0  # Extended RoPE base

# Tokenizer Configuration
tokenizer:
  type: "tiktoken"  # Llama 3 uses tiktoken
  path: "~/Library/Application Support/MLXRunner/models/Meta-Llama-3-8B-Instruct/tokenizer.json"

  # Special tokens
  bos_token: "<|begin_of_text|>"
  eos_token: "<|eot_id|>"
  pad_token: "<|end_of_text|>"

  # Additional special tokens
  special_tokens:
    - "<|start_header_id|>"
    - "<|end_header_id|>"
    - "<|eom_id|>"  # End of message

# Quantization Settings
quantization:
  dtype: "q4_k"  # FP16, Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, Q8_K
  group_size: 128  # For K-quants

# Context & Generation
context:
  max_length: 8192
  default_max_new_tokens: 1024

# RoPE Scaling (Llama 3 uses extended RoPE base)
rope_scaling:
  type: "base"  # Extended via rope_theta
  factor: 1.0

# Chat Template (Llama 3 format)
chat_template: |
  <|begin_of_text|>
  {%- if system_message %}
  <|start_header_id|>system<|end_header_id|>
  {{ system_message }}<|eot_id|>
  {%- endif %}
  <|start_header_id|>user<|end_header_id|>
  {{ user_message }}<|eot_id|>
  <|start_header_id|>assistant<|end_header_id|>

# System Prompt
default_system_prompt: "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content."

# Sampling Presets
sampling_presets:
  creative:
    temperature: 0.8
    top_p: 0.95
    top_k: 50
    repetition_penalty: 1.05

  balanced:
    temperature: 0.6
    top_p: 0.9
    top_k: 40
    repetition_penalty: 1.1

  precise:
    temperature: 0.2
    top_p: 0.85
    top_k: 20
    repetition_penalty: 1.15

# Performance Hints
performance:
  # Recommended batch size for this model
  optimal_batch_size: 8

  # Memory estimates (approximate)
  memory_fp16_mb: 16000
  memory_q4_k_mb: 5000
  memory_q8_k_mb: 9000

  # Performance targets (M4)
  target_prefill_ms: 800
  target_decode_ms: 70

# Model Metadata
metadata:
  description: "Llama 3 8B Instruct - Powerful instruction-following model"
  license: "Llama 3 Community License"
  paper: "https://ai.meta.com/blog/meta-llama-3/"
  huggingface: "meta-llama/Meta-Llama-3-8B-Instruct"
  version: "3.0"
  release_date: "2024-04"

# Tags for organization
tags:
  - "chat"
  - "instruct"
  - "llama3"
  - "gqa"
  - "extended-context"
